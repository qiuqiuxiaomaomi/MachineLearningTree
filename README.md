# MachineLearningTree
机器学习技术

![](https://i.imgur.com/5eI07Uv.png)

![](https://i.imgur.com/REkMZtc.png)

<pre>
引言：划分电影的题材类型

k-近邻算法（KNN）
    简单的说，k-近邻算法采用测量不同特征值之间的距离方法进行分类。
    优点：精度高，对异常值不敏感，无数据输入假定
    缺点：计算复杂度高，空间复杂度高
    适用数据范围：数值型和标称型

    工作原理：
       存在一个样本数据集合，也称作训练样本集，兵器样本集中每个数据都存在标签，即我们知道
       样本集中每一数据与所属分类的对应关系，输入没有标签的新数据后，将新数据的每个特征与
       样本集中数据对应特征进行比较，然后算法提取样本集中特征最相似（最近邻）的分类标签，
       一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中的k的出处，
       通常k是不大于20的整数，最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

       对于一部未知的电影，使用Python计算距离

       k-近邻算法的一般流程
           1）收集数据：可以使用任何方法
           2）准备数据：距离计算所需要的数值，最好是结构化的数据格式
           3）分析数据: 可以使用任何方法
           5）训练算法: 此步骤不适用于k-近邻算法
           6）测试算法：计算错误率
           7）使用算法：首先需要输入样本数据和结构化的输出结构，然后运行k-近邻算法判定输
              入的数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理

测试算法：机器学习算法一个很重要的工作就是评估算法的正确率，通常我们只提供已有数据的90%作为训练样本来训练分类器，
         而是用其余的10%数据来测试分类器，检测分类器的正确率。10%的测试数据应该是随机选择的。
    
          
</pre>

<pre>
决策树：
    决策树的构造
        优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
        缺点：可能会产生过度匹配问题
        适用数据类型：数值型和标称型

    在构造决策树时，需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定作用。为了找到决定性的特征，
    划分出最好的结果，我们必须评估每个特征，完成测试之后，原始数据集就被划分为几个数据子集，这些数据子集会分布在
    第一个决策点的所有分支上，如果某个分支下的数据属于同一类型，则当前无需阅读的垃圾邮件正确地划分数据分类，无需
    进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程，如何划分数据子集的
    算法和划分原始数据集的方法相同，直到所有剧痛相同类型的数据均在一个数据子集内。

    决策树的一般流程：
        1）收集数据：可以使用任何方法
        2）准备数据：树构造算法只使用与标称型数据，因此数值型数据必须离散化
        3）分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期
        5）训练算法：构造树的数据结构
        6）测试算法：使用经验树计算错误率
        7）使用算法：

    信息增益：
            划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点
        。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学，我们可以在划分数据之前使用信
        息论量化度量信息的内容。

           在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得
        的信息增益，获得信息增益最高的特征就是最好的选择。
  
        乡农定理：
             熵定义为信息的期望值，如果待分类的事务可能划分在多个分类之中，则符号xi定信息定义为
                  I(xi) = -log2p(xi)
             其中p(xi)是选择该分类的概率。 
</pre>

![](https://i.imgur.com/3AqLpbA.png)

f(x), ln(f(x))的曲线图

![](https://i.imgur.com/658ApMm.png)

<pre>
基于概率论的分类方法：朴素贝叶斯
    “数据属于哪一类”这类问题的明确答案，不过分类器有时会产生错误结果，这时可以要求分类器给出一个最优的类别猜测结果，同时
    给出这个猜测的概率估计算。

    称之为朴素，是因为整个形式化过程只做最原始，最简单的假设。

    基于朴素贝叶斯拒测理论的分类方法：
        优点：在数据较少的情况下任然有效，可以处理多类别问题
        缺点：对于输入数据的准备方式较为敏感
        使用数据类型：标称型数据

    有两类数据，p1(x,y)表示数据点（x,y）属于p1分类的概率
              p2(x,y)表示数据点（x,y）属于p2分类的概率

              1）如果p1(x,y) > p2(x,y)，那么该点属于类别p1
              2）如果p2(x,y) > p1(x,y)，那么该点属于类别p2
         贝叶斯决策理论的核心思想是选择高概率对应的类别，即选择具有最高概率的决策。
         对于这种计算类别概率的Python代码只有两行 
              1）使用KNN算法，进行1000次计算
              2）使用决策树，分别沿x轴，y轴划分数据
              3）计算数据点属于每个类别的概率，并进行比较
              
         使用决策树不会非常成功，而和简单的概率计算相比，KNN的计算量太大，因此，对于上述问题，最佳选择是使用概率比较方法
         
   使用条件概率分类：
       p(c1|x,y) 给定（x,y）表示的数据点，那么该数据点来自于分类c1的概率是多少
       p(c2|x,y) 给定(x,y)表示的数据点，那么该数据点来自于分类c2的概率是多少
             p(xi|x,y) = p(x,y|ci).p(ci)/p(x.y)
       使用定义可以定义贝叶斯分类准侧为：
          1）如果p(c1|x,y) > p(c2|x,y)，那么属于类别c1
          2）如果p(c1|x,y) < p(c1|x,y)，那么属于类别c2

   测试算法：根绝现实情况修改修改分类器
       利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算
           p(w0|1)p(w1|1)p(w2|1)，如果其中一个概率值为0，那么最后的乘积因为0，为降低这种影响，
       可以将所有词的出现数初始化为1，并将分母初始化为2

       另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积
           p(w0|ci)p(w1|ci)p(w2|ci)...p(wn|ci)时，由于大部分因子都非常小，所以程序会下溢出或者
       得到不正确的答案，一种办法是对乘积取自然对数，在代数中有ln(a * b) = ln(a) + ln(b),于是通过
       求对数可以避免下溢出或者浮点数舍入导致的错误，同时，采用自然对数进行处理不会有任何损失。
           对于f(x), ln(f(x))的曲线，检查者两条曲线，就会发现它们在相同区域内同时增加或者减少，并且
       在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。

总结：
       对于分类而言，使用概率有时要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计
    未知概率的有效方法。
       可以通过特征之间的条件独立性假设，降低对数据量的需求，独立性假设是指一个词的出现概率并不依赖于文
    档中的其他词。当然我们也知道这个假设过于简单，这就是之所以称之为朴素贝叶斯的原因，尽管条件独立性假设
    并不正确，但是朴素贝叶斯仍然是一种有效的分类器。
       利用现代编程语言来实现朴素贝叶斯时需要考虑很多实际因素。下溢出就是其中一个问题，它可以通过对概率
    取对数来解决，词袋模型在解决文档分类问题上比词集模型有所提高，还有其它一些方面的改进，比如说移除停用词，
    当然也可以化大量时间对切分器进行优化           
</pre>

Sigmoid函数

![](https://i.imgur.com/DyVDuly.png)

![](https://i.imgur.com/cVQIPr5.png)

![](https://i.imgur.com/zXFFp6N.png)

其中f(x,y)必须要在待计算的点上有定义并且可微

<pre>
Logistic回归
      假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。
   利用Logicstic回归进行分类的主要思想是：
      根据现有数据对分类边界线建立回归公式，以此进行分类，这里的“回归”一词源于最佳拟合参数。

   Logistic回归的一般过程
       1）收集数据：采用任意方法收集数据
       2）准备数据：由于需要进行距离计算，因此要求数据类型为数值型，另外，结构化数据格式最佳
       3）分析数据：采用任意方法对数据进行分析
       5）训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数
       6）测试算法：一旦训练步骤完成，分类将会很快
       7）使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；
                   接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定他们属于哪个类别；
                   之后：我们就可以在输出的类别上做一些其他分析工作

1） 基于Logistic回归和Sigmoid函数的分类
       优点：计算代价不高，易于理解和实现
       缺点：容易欠拟合，分类精度可能不高
       使用数据类型：数值型和标称型数据

    Sigmoid函数：
       当x为0时，Sigmoid函数值为0.5。随着x的增大，对应的Sigmoid值将逼近于1；而随着x的减小，Sigmoid值将逼近于0，如果横坐标可读足够大，Sigmoid函数看起来很像一个跃阶函数。
          因此为了实现Logistic回归分类器，可以再没给特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和带入Sigmoid中，
       进而得到一个范围在0~1之间的数值，任何大于0.5的数据被分入1类，小于0.5的值即被分入0类。所以Logistic回归也可以被看成是一种
       概率估计

    1）基于最优化方法的最佳回归系数确定
       Sigmoid函数的输入记为z,由下面的公式得出：
              z = w0x0 + w1x1 + w2x2 + ... + wnxn

       梯度上升法：
           梯度上升法基于的思想就是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻，如果梯度记为V, 则函数f(x,y)
       的梯度由下标识
           梯度上升的算法来求函数的最大值，而梯度下降算法用来求函数的最小值。
           
   Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法
   来完成，在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。
   随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源，此外，随机梯度上升是一个在线算法，他可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批处理
   运算。
   机器学习的一个重要问题就是如何处理缺失数据，这个问题没有标准答案，取决于实际应用中的需求
</pre>

![](https://i.imgur.com/osjOCYL.png)

<pre>
支持向量机
   将数据集分隔开来的直线称为超平面，如果数据点都在二维平面上，所以此时分隔超平面就只是
   一条直线，但是如果所给的数据集是三维的，那么此时用来分隔数据的就是一个平面，显而易见，
   更高纬的情况可以以此类推，如果数据集是1024维的，那么久需要一个1023维的某某对象来对
   数据进行分隔，这个1023维的某某对象被称为超平面，也就是分类的决策边界，分布在超平面
   一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。

   我们希望采用这种方式来构建分类器，即如果数据点离决策边界越远，那么其最后的越策结果就
   越可信。

   我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能原，这里点到分割面的
   距离被称为间隔，我们希望间隔尽可能地大，这是因为如果犯错或者在有限数据上训练分类器
   的话，我们希望分类器尽可能健壮
 
   支持向量就是离分隔超平面最近的那些点，
</pre>

<pre>
利用AdaBoost元素法提高分类性能
</pre>

<pre>
预测数值型数据：回归
</pre>

<pre>
树回归
</pre>

<pre>
利用K-均值聚类算法对未标注数据分组
</pre>

<pre>
使用Apriori算法进行关联
</pre>

<pre>
使用FP-growth算法来高效发现频繁项集
</pre>

<pre>
利用PCA来简化数据
</pre>

<pre>
利用SVD简化数据
</pre>

<pre>
大数据与MapReduce
</pre>
